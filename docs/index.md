# About me

<figure markdown="1" style="margin:0 auto; text-align: center;">
![Image title](images/site-images/image-modified.png){ width="200" }
</figure>

<p style="text-align: center; margin-top:-10px;"><b>Naresh Kumar Devulapally</b></p>

<p style="text-align: center;margin-top:-10px;">| <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Linkedin1</a> | <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Github</a> | <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Resume</a> |</p>

## News

!!! info ""

    **Fall 2023**:

    - M.S. Thesis Candidate co-advised by Prof. [Sreyasee Das Bhattacharjee](https://cse.buffalo.edu/~sreyasee/) and Prof. [Junsong Yuan](https://cse.buffalo.edu/~jsyuan/).
    - Graduate Teaching Assistant @ (CSE 574) Machine Learning course at UB.

    **Summer 2023**:

    - **<u>One Paper accepted at ACM Multimedia 2023</u>**.

    **Spring 2023**:

    - <u>Submitted two papers to ACM Multimedia 2023</u>.
    - Graduate Teaching Asssistant @ (CSE 560) Data Models and Query Languages course at UB.

    **Fall 2022**:

    - Supervised Researcher under the guidance of Professor Sreyasee Das Bhattacharjee and Professor Junsong Yuan. <u>Submitted one paper to IJCAI 2023.</u>
    - Graduate Teaching Asssistant @ (CSE 555) Introduction to Pattern Recognition at UB.

## Research Projects

**Multi-label Emotion Analysis in Conversation via Multimodal
Knowledge Distillation.**

<div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/acm-prop.png" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">ArXiV</a> | <a href= "#">PDF</a> |</p>
</div>
      
<p style="margin-top: 0;">
We present Self-supervised Multi-Label Peer Collaborative Distillation (SeMuL-PCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously. The proposed Multimodal Distillation Loss calibrates the fusion network by minimizing the Kullback–Leibler divergence with the peer networks. Additionally, each peer network is conditioned using a self-supervised contrastive objective to improve the generalization across diverse socio-demographic speaker backgrounds. We out-perform the current state-of-the-art models on several large-scale public datasets (MOSEI, EmoReact and ElderReact) with 17% improvement in weighted F1-score during cross-dataset experimental settings. We demonstrate impressive generalization ability across age and demography-diverse populations.
    </p>
  
  </div>
---
**Privacy-preserving Multi-modal Attentive Learning framework for real-time emotion tracking in conversations.**

<div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/primal.PNG" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">ArXiV</a> | <a href= "#">Paper</a> |</p>
</div>
      
<p>
      We propose a PRIvacy-preserving Multimodal Attentive Learning framework (PRIMAL) that derives the person independent normalized facial Action-Unit based features to estimate the participants’ expression and keeps track of their spatio-temporal states and conversation dynamics in context of their surrounding environment to evaluate the speaker emotion. By designing a novel contrastive loss based optimization framework to capture the self- and cross-modal correlation within a learned descriptor, PRIMAL exhibits promise in accurately identifying the emotion state of an individual speaker in group conversations.
    </p>
  
  </div>

---

**Adaptive Multimodal Fusion for
Tracking Speaker Emotion in Group Conversations.**

<div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/amus.png" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">ArXiV</a> | <a href= "#">PDF</a> |</p>
</div>
      
<p style="margin-top: 0;">
      We propose a Multimodal framework that captures cross-modal interactions at various levels of spatial abstraction details by employing a Hierarchical Attention Network (HAN) that administers the cross-modal attention via Peripheral key-value pairs within each layer of a mode-specific Central query network to ensure the fusion process to occur at various levels of spatial details. The resulting cross-attended mode-specific descriptors are combined by means of an Adaptive Multimodal fuSion (AMuS) protocol to uncover the unique and novel instance-specific patterns within the learned multimodal utterance descriptor.
    </p>
  
  </div>

## Academic Projects

Projects done as a part of the courses at UB.
