# About me

<figure markdown="1" style="margin:0 auto; text-align: center;">
![Image title](images/site-images/image-modified.png){ width="200" }
</figure>

<p style="text-align: center; margin-top:-10px; font-size: 22px;"><b>Naresh Kumar Devulapally</b></p>

<p style="text-align: center;margin-top:-25px;"><a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">LinkedIn</a> | <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Github</a> | <a href="https://www.linkedin.com/in/nareshdevulapally" target = "_blank">Resume</a></p>

I am an MS Thesis candidate co-advised by <a href="https://cse.buffalo.edu/~jsyuan/" target = "_blank">Dr. Junsong Yuan</a> and <a href="https://cse.buffalo.edu/~sreyasee/" target = "_blank">Dr. Sreyasee Das Bhattacharjee</a> at The Visual Computing Lab in the Department of Computer Science at University at Buffalo, SUNY. **My Research is focused on Computer Vision and Multimodal AI (Vision-Language models)** with applications towards Emotion Recognition, Crowd Analytics, Transformer based Object Understanding, and Emotion Generation. My recent (first-author) works are accepted at ACM Multimedia 2023 (∼25% acceptance rate) and BigMM 2023.

<hr/>

!!! info "Research Updates"
    <!-- <p style="text-align: center; margin-top:-5px; font-size: 18px;"><b>Research Updates</b></p> -->
    **Fall 2023**:

    - **<u>One Paper accepted at BigMM 2023</u>**.
    - M.S. Thesis Candidate co-advised by Prof. [Junsong Yuan](https://cse.buffalo.edu/~jsyuan/) and Prof. [Sreyasee Das Bhattacharjee](https://cse.buffalo.edu/~sreyasee/).
    - Graduate Teaching Assistant @ (CSE 574) Machine Learning course at UB.

    **Summer 2023**:

    - **<u>One Paper accepted at ACM Multimedia 2023</u>**.
    - Graduate Teaching Assistant @ (CSE 701) Sport Analysis using Computer Vision seminar.

    **Spring 2023**:

    - <u>Submitted two papers to ACM Multimedia 2023</u>.
    - Graduate Teaching Asssistant @ (CSE 560) Data Models and Query Languages course at UB.

    **Fall 2022**:

    - Supervised Researcher under the guidance of Professor Sreyasee Das Bhattacharjee and Professor Junsong Yuan. <u>Submitted one paper to IJCAI 2023.</u>
    - Graduate Teaching Asssistant @ (CSE 555) Introduction to Pattern Recognition at UB.

## Research Projects

<!-- <div>
<div style="float:center; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/acm-prop.png" alt="Longtail boat in Thailand" width="500">

<p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">ArXiV</a> | <a href= "#">PDF</a> |</p>
</div> -->
!!! info "[Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation.](conferences/mmfp3687-anand.md)"
    <figure markdown="1" style="margin:0 auto; text-align: center;">
    ![Image title](images/site-images/acm-prop.png){ width="600" }
    </figure>
    
    <center>[Something](https://www.linkedin.com/in/nareshdevulapally)</center>

    <p style="margin-top: -10px;">
    We present Self-supervised Multi-Label Peer Collaborative Distillation (SeMuL-PCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously.
    </p>

!!! info "[Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation.](conferences/mmfp3687-anand.md)"
    <figure markdown="1" style="margin:0 auto; text-align: center;">
    ![Image title](images/site-images/acm-prop.png){ width="600" }
    </figure>
    
    <center>[Something](https://www.linkedin.com/in/nareshdevulapally)</center>

    <p style="margin-top: -10px;">
    We present Self-supervised Multi-Label Peer Collaborative Distillation (SeMuL-PCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously.
    </p>
---
**Privacy-preserving Multi-modal Attentive Learning framework for real-time emotion tracking in conversations.**

<div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/primal.PNG" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">ArXiV</a> | <a href= "#">Paper</a> |</p>
</div>

      
<p>
      We propose a PRIvacy-preserving Multimodal Attentive Learning framework (PRIMAL) that derives the person independent normalized facial Action-Unit based features to estimate the participants’ expression and keeps track of their spatio-temporal states and conversation dynamics in context of their surrounding environment to evaluate the speaker emotion. By designing a novel contrastive loss based optimization framework to capture the self- and cross-modal correlation within a learned descriptor, PRIMAL exhibits promise in accurately identifying the emotion state of an individual speaker in group conversations.
    </p>
  
  </div>

---

**Adaptive Multimodal Fusion for
Tracking Speaker Emotion in Group Conversations.**

<div>
<div style="float:left; margin-top:5px; padding-right: 18px">
<img src="../images/site-images/amus.png" alt="Longtail boat in Thailand" width="350"><p style="margin: 0; padding:0; text-align:center">| <a href= "#">GitHub</a> | <a href= "#">ArXiV</a> | <a href= "#">PDF</a> |</p>
</div>
      
<p style="margin-top: 0;">
      We propose a Multimodal framework that captures cross-modal interactions at various levels of spatial abstraction details by employing a Hierarchical Attention Network (HAN) that administers the cross-modal attention via Peripheral key-value pairs within each layer of a mode-specific Central query network to ensure the fusion process to occur at various levels of spatial details. The resulting cross-attended mode-specific descriptors are combined by means of an Adaptive Multimodal fuSion (AMuS) protocol to uncover the unique and novel instance-specific patterns within the learned multimodal utterance descriptor.
    </p>
  
  </div>

## Academic Projects

Projects done as a part of the courses at UB.
