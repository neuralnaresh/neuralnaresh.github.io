{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About me","text":"<p>Naresh Kumar Devulapally</p> <p>Google Scholar | LinkedIn | Github | CV</p> <p>I am a Computer Science Ph.D. candidate (Chair's Fellowship) with research interests in Multimodal Machine Learning, Representation Learning, and Graph Embeddings. My research advised by Dr. Junsong Yuan and Dr. Sreyasee Das Bhattacharjee at The Visual Computing Lab in the Department of Computer Science at the University at Buffalo, SUNY. My recent (first-author) works on Multimodal Machine Learning are published at ACM Multimedia 2023 (\u223c25% acceptance rate), ICME 2024, BigMM 2023 and so on.</p> <p>Research Updates</p> <p>Spring 2024:</p> <ul> <li>\ud83c\udf89 One Paper on Multimodal Machine Learning, accepted at ICME 2024.</li> <li>Graduate Teaching Assistant @ (CSE 560) Data Models and Query Languages course at UB.</li> </ul> <p> Fall 2023:</p> <ul> <li>\ud83c\udf89 One Paper on Adaptive Fusion for Multimodal ML models, accepted at BigMM 2023.</li> <li>Graduate Teaching Assistant @ (CSE 574) Machine Learning course at UB.</li> </ul> <p>Summer 2023:</p> <ul> <li>\ud83c\udf89 One Paper on Representation Learning, Explainability in Multimodal systems, accepted at ACM Multimedia 2023.</li> <li>Graduate Teaching Assistant @ (CSE 701) Sport Analysis using Computer Vision seminar.</li> </ul> <p>Spring 2023:</p> <ul> <li>Submitted two papers to ACM Multimedia 2023.</li> <li>Graduate Teaching Assistant @ (CSE 560) Data Models and Query Languages course at UB.</li> </ul> <p>Fall 2022:</p> <ul> <li>Submitted one paper to IJCAI 2023.</li> <li>Graduate Teaching Assistant @ (CSE 555) Introduction to Pattern Recognition at UB.</li> </ul>"},{"location":"#research-publications","title":"Research Publications","text":"<p>Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning.</p> <p> </p> <p>PDF | Code | Poster | Web Page</p> <p><p> We propose AM\\(^2\\)-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. Second, the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By reporting around 2-5% improvement in the weighted-F1 score, the proposed multimodal joint embedding module facilitates an impressive performance gain in a variety of missing-modality query scenarios during test time. </p></p> <p>Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation.</p> <p> </p> <p>PDF | Code | Poster | Web Page</p> <p><p> We present Self-supervised Multi-Label Peer Collaborative Distillation (SeMuL-PCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously. </p></p> <p>AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in Group Conversations.</p> <p> </p> <p>PDF | Code | Poster | Web Page</p> <p><p> We propose a Multimodal Attention Network (MAN) that captures cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of mode-specific Peripheral and Central networks. The proposed MAN \u201cinjects\u201d cross-modal attention via its Peripheral key-value pairs within each layer of a mode-specific Central query network. The resulting cross-attended mode-specific descriptors are then combined using an Adaptive Fusion (AF) technique that enables the model to integrate the  discriminative and complementary mode-specific data patterns within an instance-specific multimodal descriptor. </p></p> <p>Privacy-preserving Multi-modal Attentive Learning framework for real-time emotion tracking in conversations.</p> <p> </p> <p>PDF | Code | Poster | Web Page</p> <p><p> We propose a PRIvacy-preserving Multimodal Attentive Learning framework (PRIMAL) that derives the person-independent normalized facial Action-Unit based features to estimate the participants\u2019 expression and keep track of their spatiotemporal states and conversation dynamics in the context of their surrounding environment to evaluate the speaker's emotion. By designing a novel contrastive loss-based optimization framework to capture the self- and cross-modal correlation within a learned descriptor, PRIMAL exhibits promise in accurately identifying the emotional state of an individual speaker in group conversations. </p></p>"},{"location":"#academic-projects","title":"Academic Projects","text":"<p>Projects are done as a part of the courses at UB.</p>"},{"location":"conferences/mmfp3687-anand/","title":"Multi-label Emotion Analysis in Conversation via Multimodal Knowledge Distillation","text":"<p>Proposed Architecture of the SeMuL-PCD model.</p> <p>PDF | Code | Poster</p> <p>Evaluating speaker emotion in conversations is crucial for various applications requiring human-computer interaction. However, co-occurrences of multiple emotional states (e.g. \u2018anger\u2019 and \u2018frustration\u2019 may occur together or one may influence the occurrence of the other) and their dynamic evolution may vary dramatically due to the speaker\u2019s internal (e.g., influence of their personalized socio-cultural-educational and demographic backgrounds) and external contexts. Thus far, the previous focus has been on evaluating only the dominant emotion observed in a speaker at a given time, which is susceptible to producing misleading classification decisions for difficult multi-labels during testing. In this work, we present Self-supervised Multi-Label Peer Collaborative Distillation (SeMuLPCD) Learning via an efficient Multimodal Transformer Network, in which complementary feedback from multiple mode-specific peer networks (e.g.transcript, audio, visual) are distilled into a single mode-ensembled fusion network for estimating multiple emotions simultaneously. The proposed Multimodal Distillation Loss calibrates the fusion network by minimizing the Kullback\u2013Leibler divergence with the peer networks. Additionally, each peer network is conditioned using a self-supervised contrastive objective to improve the generalization across diverse socio-demographic speaker backgrounds. By enabling peer collaborative learning that allows each network to independently learn their mode-specific discriminative patterns, SeMUL-PCD is effective across different conversation environments. In particular, the model not only outperforms the current state-of-the-art models on several large-scale public datasets (e.g., MOSEI, EmoReact and ElderReact), but with around 17% improved weighted F1-score in the cross-dataset experimental settings. The model also demonstrates an impressive generalization ability across age and demography-diverse populations.</p>"}]}